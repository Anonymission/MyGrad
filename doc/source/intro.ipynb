{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing MyGrad\n",
    "MyGrad is a simple, NumPy-centric autograd library. An autograd library enables you to automatically compute derivatives of mathematical functions. This library\n",
    "is designed to serve primarily as an education tool for learning about gradient-based machine learning; it is easy to install, has a readable and easily customizable code\n",
    "base, and provides a sleek interface that mimics NumPy. Furthermore, it leverages NumPy's vectorization\n",
    "to achieve good performance despite the library's simplicity.\n",
    "\n",
    "This is not meant to be a competitor to libraries like PyTorch (which `mygrad` most closely resembles) or\n",
    "TensorFlow. Rather, it is meant to serve as a useful tool for students who are learning about training neural networks\n",
    "using back propagation.\n",
    "\n",
    "## Installing MyGrad\n",
    "To install MyGrad clone [this repository](https://github.com/rsokl/MyGrad) and navigate to the MyGrad directory, then run:\n",
    "```shell\n",
    "python setup.py install\n",
    "```\n",
    "\n",
    "MyGrad requires `numpy`. It is highly recommended that you utilized `numpy` built with [MKL](https://en.wikipedia.org/wiki/Math_Kernel_Library)\n",
    " for access to optimized math routines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Application\n",
    "Let's use `mygrad` to compute the derivative of\n",
    "$f(x) = x^2$ evaluated at $x = 3$ (which is $\\frac{df}{dx}\\rvert_{x=3} = 2\\times 3$).\n",
    "\n",
    "`mygrad.Tensor` behaves nearly identically to NumPy's ndarray, in addition to having the machinery needed to\n",
    "compute the analytic derivatives of functions. Suppose we want to compute this derivative at `x = 3`. We can create a\n",
    "0-dimensional tensor (a scalar) for x and compute `f(x)`:\n",
    "\n",
    "```python\n",
    ">>> import mygrad as mg\n",
    ">>> x = mg.Tensor(3.0)\n",
    ">>> f = x ** 2\n",
    ">>> f\n",
    "Tensor(9.0)\n",
    "```\n",
    "\n",
    "Invoking `f.backward()` instructs `mygrad` to trace through the computational graph that produced `f` and compute the\n",
    "derivatives of `f` with respect to all of its independent variables. Thus, executing `f.backward()` will compute $\\frac{df}{dx} = 2x$ at $x=3$, and will store the resulting value in `x.grad`:\n",
    "\n",
    "```python\n",
    ">>> f.backward()  # triggers computation of `df/dx`\n",
    ">>> x.grad  # df/dx = 2x = 6.0\n",
    "array(6.0)\n",
    "```\n",
    "\n",
    "This is the absolute tip of the iceberg. `mygrad` can compute derivatives of multivariable composite\n",
    "functions of tensor-valued variables!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Some Bells and Whistles\n",
    "`mygrad` supports all of NumPy's essential features, including:\n",
    " - [N-dimensional tensors](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/IntroducingTheNDarray.html) that can be reshaped and have their axes transposed\n",
    " - [vectorization](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/VectorizedOperations.html)\n",
    " - [broadcasting](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/Broadcasting.html)\n",
    " - [basic and advanced indexing](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/BasicIndexing.html) (including all varieties of mixed indexing schemes) for both getting and setting items.\n",
    " - fully-fledged support for [einsum](https://rockt.github.io/2018/04/30/einsum) (including broadcasting and traces,\n",
    "   which are not supported by PyTorch, TensorFlow, or HIPS-autograd)\n",
    "\n",
    " `mygrad.Tensor` plays nicely with NumPy-arrays, which behave as constants when they are used in computational graphs:\n",
    "\n",
    "```python\n",
    ">>> import numpy as np\n",
    ">>> x = mg.Tensor([2.0, 2.0, 2.0])\n",
    ">>> y = np.array([1.0, 2.0, 3.0])\n",
    ">>> f = x ** y  # (2 ** 1, 2 ** 2, 2 ** 3)\n",
    ">>> f.backward()\n",
    ">>> x.grad\n",
    "array([ 1.,  4., 12.])\n",
    "```\n",
    "\n",
    "`mygrad.nn` supplies essential functions for machine learning, including:\n",
    "- N-dimensional convolutions (with striding, dilation, and padding)\n",
    "- N-dimensional pooling\n",
    "- A [gated recurrent unit](https://en.wikipedia.org/wiki/Gated_recurrent_unit) for sequence-learning (with input-level\n",
    "  dropout and variational hidden-hidden dropout)\n",
    "\n",
    "It leverages a nice [sliding window\n",
    "view](https://github.com/rsokl/MyGrad/blob/a72ebc26acf5c254f59a562c8045698387763a41/mygrad/nnet/layers/utils.py#L6)\n",
    "function, which produces convolution-style windowed views of arrays/tensors without making copies of them, to\n",
    "intuitively (and quite efficiently) perform the neural network-style convolutions and pooling.\n",
    "\n",
    "## Advanced Example\n",
    "The following is an example of using `mygrad` to compute the [hinge loss](https://en.wikipedia.org/wiki/Hinge_loss) of classification scores and to \"backpropagate\" through (compute the gradient of) this loss. This example demonstrates some of mygrad's ability to perform backpropagation through broadcasted operations, basic indexing, advanced indexing, and in-place assignments.\n",
    "\n",
    "```python\n",
    ">>> from mygrad import Tensor\n",
    ">>> import numpy as np\n",
    ">>> class_scores = Tensor(10 * np.random.rand(100, 10))         # 100 samples, 10 possible classes for each\n",
    ">>> class_labels = np.random.randint(low=0, high=10, size=100)  # correct label for each datum\n",
    ">>> class_labels = (range(len(class_labels)), class_labels)\n",
    ">>> correct_class_scores = class_scores[class_labels]\n",
    "\n",
    ">>> Lij = class_scores - correct_class_scores[:, np.newaxis] + 1.  # 100x10 margins\n",
    ">>> Lij[Lij <= 0] = 0      # scores within the hinge incur no loss\n",
    ">>> Lij[class_labels] = 0  # the score corresponding to the correct label incurs no loss\n",
    "\n",
    ">>> loss = Lij.sum() / class_scores.shape[0]  # compute mean hinge loss\n",
    ">>> loss.backward()    # compute gradient of loss w.r.t all dependent tensors\n",
    ">>> class_scores.grad  # d(loss)/d(class_scores)\n",
    "array([[ 0.  ,  0.01,  0.  , -0.04,  0.  ,  0.  ,  0.01,  0.  ,  0.01, 0.01], ...])\n",
    "```\n",
    "\n",
    "## Computational Graph Visualization\n",
    "MyGrad provides the capability to visually render diagrams of your computational graphs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mygrad as mg\n",
    "from mygrad.computational_graph import build_graph\n",
    "x = mg.Tensor(2)\n",
    "y = mg.Tensor(3)\n",
    "f = x * y\n",
    "g = f + x - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`mygrad` uses [Graphviz](http://www.graphviz.org) and a [Python interface for Graphviz](https://graphviz.readthedocs.io/en/stable/) to render the computational graphs built using tensors. These graphs can be rendered in Jupyter notebooks, allowing for quick checks of graph structure, or can be saved to file for later reference.\n",
    "\n",
    "The dependencies can be installed with:\n",
    "\n",
    "```shell\n",
    "conda install graphviz\n",
    "conda install python-graphviz\n",
    "```\n",
    "\n",
    "Big thanks to [Petar Griggs](https://github.com/petarmhg) for implementing these fantastic viz capabilities!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
