

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>MyGrad &mdash; MyGrad 0+unknown documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/my_theme.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Installing MyGrad" href="install.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="#" class="icon icon-home"> MyGrad
          

          
          </a>

          
            
            
              <div class="version">
                0+unknown
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installing MyGrad</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html">Introducing MyGrad</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">MyGrad’s Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="operation.html">MyGrad’s Operation Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_creation.html">Tensor creation routines (<code class="docutils literal notranslate"><span class="pre">mygrad.tensor_creation</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_manipulation.html">Tensor manipulation routines (<code class="docutils literal notranslate"><span class="pre">mygrad.tensor_manip</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">Linear algebra (<code class="docutils literal notranslate"><span class="pre">mygrad.linalg</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="math.html">Mathematical functions (<code class="docutils literal notranslate"><span class="pre">mygrad.math</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="nnet.html">Neural network operations (<code class="docutils literal notranslate"><span class="pre">mygrad.nnet</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="graph_viz.html">Computational graph visualization(<code class="docutils literal notranslate"><span class="pre">mygrad.computational_graph</span></code>)</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">MyGrad</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="#">Docs</a> &raquo;</li>
        
      <li>MyGrad</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="mygrad">
<h1>MyGrad<a class="headerlink" href="#mygrad" title="Permalink to this headline">¶</a></h1>
<p>MyGrad is a simple, NumPy-centric math library that is capable of performing <em>automatic differentiation</em>. That is, the
mathematical functions provided by MyGrad are capable of computing their own derivatives. If you know <a class="reference external" href="https://www.pythonlikeyoumeanit.com/module_3.html">how to use NumPy</a> then you can learn how to use MyGrad in a matter of minutes!</p>
<p>While fantastic auto-differentiation libraries like TensorFlow, PyTorch, and MXNet are available to the same end as
MyGrad (and far beyond, ultimately), they are industrial-grade tools in both function and form. MyGrad’s primary purpose
is to serve as an educational tool. It is simple to install (its only core dependency in NumPy), it is trivial to use
if you are comfortable with NumPy, and its code base is well-documented and easy to understand. This makes it simple for
students and teachers alike to use, hack, prototype with, and enhance MyGrad!</p>
<div class="section" id="why-is-automatic-differentiation-useful">
<h2>Why is Automatic Differentiation Useful?<a class="headerlink" href="#why-is-automatic-differentiation-useful" title="Permalink to this headline">¶</a></h2>
<p>In general, auto-differentiation permits us to compute massive equations that depend on millions of variables and then
seamlessly evaluate the derivatives of the equation’s output <em>with respect to every one of those variables</em>. This
capability lies at the heart of the burgeoning field of <strong>deep learning</strong>, which is now the predominant use case for
auto-differentiation libraries, and is the manifest purpose of TensorFlow, PyTorch, and MXNet.</p>
<p>The “decisions” made by a neural network are dictated by the network’s many, many parameters, which us researchers have
arranged to serve as variables in a tremendous equation. This equation might, for example, attempt to take as input the pixels
of a picture and return as an output an image-classification - a statement of the image’s content (e.g. 0 is ‘dog’,
1 is ‘cat’, etc.).</p>
<p>The way that we train this neural network is by “tuning” the values of its many parameters so that the network’s
predictions reliably agree with what we know to be true. It turns out that having access to the derivative of the
neural network’s output with respect to its parameters grants us the ability to quite reliably optimize its parameters -
through a process known as gradient-based optimization we can update the values of these parameters to steer the neural
network towards making more faithful predictions (note: a gradient it just a collection of derivatives of a
multivariate function).</p>
<p>More specifically, we can hook our neural network up to an “objective” function that measures how well its predictions
match against “the truth”. Recalling the basic definition of a derivative (as prescribed by any calculus course) and its
relationship to the slope of a function at a point, knowing
the derivative of this objective function with respect to one of our neural network’s parameters means that we know whether
increasing this parameter will increase or decrease the output of the objective function; tuning the parameter so will affect
the network’s output such that its prediction is in closer agreement with the truth than before. If we make such an
adjustment to each of our neural network’s parameters and repeat this process many times over, using a wide variety of
“training data” we may arrive at a configuration of network parameters that permits our neural network to faithfully
classify pictures that we have never encountered before.</p>
<p>Thus auto-differentiation permits us to efficiently and automatically compute the derivatives of massive functions by
way of simply coding the functions using the auto-differentiation software. This in turn, is what allows us nimbly design
neural networks and objective functions, and to tune the parameters of our neural networks using derivative-based (or
gradient-based) optimization schemes.</p>
<p>It should be noted that description of training neural networks, as presented here, only provides a narrow view of deep learning.
Specifically, it describes the supervised learning of an image classification problem. While this is sufficient for conveying
the utility of auto-differentiation software as a means for training neural networks, there is more nuiance to deep learning
than is suggested here.</p>
<div class="toctree-wrapper compound">
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installing MyGrad</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html">Introducing MyGrad</a><ul>
<li class="toctree-l2"><a class="reference internal" href="intro.html#Installing-MyGrad">Installing MyGrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#A-Simple-Application">A Simple Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#Some-Bells-and-Whistles">Some Bells and Whistles</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#Advanced-Example">Advanced Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#Computational-Graph-Visualization">Computational Graph Visualization</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">MyGrad’s Tensor</a><ul>
<li class="toctree-l2"><a class="reference internal" href="tensor.html#creating-a-tensor">Creating a Tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensor.html#forward-and-back-propagation">Forward and Back-Propagation</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensor.html#accessing-the-underlying-numpy-array">Accessing the Underlying NumPy Array</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensor.html#documentation-for-mygrad-tensor">Documentation for mygrad.Tensor</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="operation.html">MyGrad’s Operation Class</a><ul>
<li class="toctree-l2"><a class="reference internal" href="operation.html#explaining-scalar-only-operations">Explaining Scalar-Only Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="operation.html#documentation-for-mygrad-operation">Documentation for mygrad.Operation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tensor_creation.html">Tensor creation routines (<code class="docutils literal notranslate"><span class="pre">mygrad.tensor_creation</span></code>)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="tensor_creation.html#ones-and-zeros">Ones and zeros</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensor_creation.html#numerical-ranges">Numerical ranges</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tensor_manipulation.html">Tensor manipulation routines (<code class="docutils literal notranslate"><span class="pre">mygrad.tensor_manip</span></code>)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="tensor_manipulation.html#changing-array-shape">Changing array shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensor_manipulation.html#transpose-like-operations">Transpose-like operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensor_manipulation.html#changing-number-of-dimensions">Changing number of dimensions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">Linear algebra (<code class="docutils literal notranslate"><span class="pre">mygrad.linalg</span></code>)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="linalg.html#matrix-and-vector-products">Matrix and vector products</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="math.html">Mathematical functions (<code class="docutils literal notranslate"><span class="pre">mygrad.math</span></code>)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="math.html#trigonometric-functions">Trigonometric functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="math.html#hyperbolic-functions">Hyperbolic functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="math.html#sums-products-differences">Sums, products, differences</a></li>
<li class="toctree-l2"><a class="reference internal" href="math.html#exponents-and-logarithms">Exponents and logarithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="math.html#other-special-functions">Other special functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="math.html#arithmetic-operations">Arithmetic operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="math.html#miscellaneous">Miscellaneous</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="nnet.html">Neural network operations (<code class="docutils literal notranslate"><span class="pre">mygrad.nnet</span></code>)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="nnet.html#layer-operations">Layer operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnet.html#losses">Losses</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnet.html#activations">Activations</a></li>
<li class="toctree-l2"><a class="reference internal" href="nnet.html#sliding-window-view-utility">Sliding Window View Utility</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="graph_viz.html">Computational graph visualization(<code class="docutils literal notranslate"><span class="pre">mygrad.computational_graph</span></code>)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/mygrad.computational_graph.build_graph.html">mygrad.computational_graph.build_graph</a></li>
</ul>
</li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="install.html" class="btn btn-neutral float-right" title="Installing MyGrad" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Ryan Soklaski

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>