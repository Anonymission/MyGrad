

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Introducing mygrad &mdash; MyGrad 0+unknown documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/my_theme.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> MyGrad
          

          
          </a>

          
            
            
              <div class="version">
                0+unknown
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installing MyGrad</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html">Introducing MyGrad</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">MyGrad’s Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="operation.html">MyGrad’s Operation Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_creation.html">Tensor creation routines (<code class="docutils literal notranslate"><span class="pre">mygrad.tensor_creation</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_manipulation.html">Tensor manipulation routines (<code class="docutils literal notranslate"><span class="pre">mygrad.tensor_manip</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">Linear algebra (<code class="docutils literal notranslate"><span class="pre">mygrad.linalg</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="math.html">Mathematical functions (<code class="docutils literal notranslate"><span class="pre">mygrad.math</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="nnet.html">Neural network operations (<code class="docutils literal notranslate"><span class="pre">mygrad.nnet</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="graph_viz.html">Computational graph visualization(<code class="docutils literal notranslate"><span class="pre">mygrad.computational_graph</span></code>)</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MyGrad</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Introducing mygrad</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/intro_2.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="introducing-mygrad">
<span id="introducing-mygrad"></span><h1>Introducing mygrad<a class="headerlink" href="#introducing-mygrad" title="Permalink to this headline">¶</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">mygrad</span></code> is a simple, NumPy-centric autograd library. An autograd library enables you to automatically compute derivatives of mathematical functions. This library
is designed to serve primarily as an education tool for learning about gradient-based machine learning; it is easy to install, has a readable and easily customizable code
base, and provides a sleek interface that mimics NumPy. Furthermore, it leverages NumPy’s vectorization
to achieve good performance despite the library’s simplicity.</p>
<p>This is not meant to be a competitor to libraries like PyTorch (which <code class="docutils literal notranslate"><span class="pre">mygrad</span></code> most closely resembles) or
TensorFlow. Rather, it is meant to serve as a useful tool for students who are learning about training neural networks
using back propagation.</p>
<div class="section" id="installing-mygrad">
<span id="installing-mygrad"></span><h2>Installing mygrad<a class="headerlink" href="#installing-mygrad" title="Permalink to this headline">¶</a></h2>
<p>To install MyGrad clone <a class="reference external" href="https://github.com/rsokl/MyGrad">this repository</a> and navigate to the MyGrad directory, then run:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python setup.py install
</pre></div>
</div>
<p>MyGrad requires <code class="docutils literal notranslate"><span class="pre">numpy</span></code>. It is highly recommended that you utilized <code class="docutils literal notranslate"><span class="pre">numpy</span></code> built with <a class="reference external" href="https://en.wikipedia.org/wiki/Math_Kernel_Library">MKL</a>
for access to optimized math routines.</p>
</div>
<div class="section" id="a-simple-application">
<span id="a-simple-application"></span><h2>A Simple Application<a class="headerlink" href="#a-simple-application" title="Permalink to this headline">¶</a></h2>
<p>Let’s use <code class="docutils literal notranslate"><span class="pre">mygrad</span></code> to compute the derivative of
<img alt="CodeCogsEqn.gif" src="https://user-images.githubusercontent.com/29104956/39901776-9e5ed362-5498-11e8-9890-e84aa2b6dae1.gif" />,
which is <a href="https://www.codecogs.com/eqnedit.php?latex=df/dx&space;=&space;2x" target="_blank"><img
src="https://latex.codecogs.com/gif.latex?df/dx&space;=&space;2x" title="\frac{df}{dx} = 2x"
/></a>.</p>
<p><code class="docutils literal notranslate"><span class="pre">mygrad.Tensor</span></code> behaves nearly identically to NumPy’s ndarray, in addition to having the machinery needed to
compute the analytic derivatives of functions. Suppose we want to compute this derivative at <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">3</span></code>. We can create a
0-dimensional tensor (a scalar) for x and compute <code class="docutils literal notranslate"><span class="pre">f(x)</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mygrad</span> <span class="kn">as</span> <span class="nn">mg</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">mg</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span>
<span class="go">Tensor(9.0)</span>
</pre></div>
</div>
<p>Invoking <code class="docutils literal notranslate"><span class="pre">f.backward()</span></code> instructs <code class="docutils literal notranslate"><span class="pre">mygrad</span></code> to trace through the computational graph that produced <code class="docutils literal notranslate"><span class="pre">f</span></code> and compute the
derivatives of <code class="docutils literal notranslate"><span class="pre">f</span></code> with respect to all of its independent variables. Thus, executing <code class="docutils literal notranslate"><span class="pre">f.backward()</span></code> will compute <a
href="https://www.codecogs.com/eqnedit.php?latex=df/dx&" target="_blank"><img
src="https://latex.codecogs.com/gif.latex?df/dx&" title="\frac{df}{dx}" /></a> and will store the value in
<code class="docutils literal notranslate"><span class="pre">x.grad</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># triggers computation of `df/dx`</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">grad</span>  <span class="c1"># df/dx = 2x = 6.0</span>
<span class="go">array(6.0)</span>
</pre></div>
</div>
<p>This is the absolute tip of the iceberg. <code class="docutils literal notranslate"><span class="pre">mygrad</span></code> can compute derivatives of multivariable composite
functions of tensor-valued variables!</p>
</div>
<div class="section" id="some-bells-and-whistles">
<span id="some-bells-and-whistles"></span><h2>Some Bells and Whistles<a class="headerlink" href="#some-bells-and-whistles" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">mygrad</span></code> supports all of NumPy’s essential features, including:</p>
<ul class="simple">
<li><a class="reference external" href="https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/IntroducingTheNDarray.html">N-dimensional tensors</a> that can be reshaped and have their axes transposed</li>
<li><a class="reference external" href="https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/VectorizedOperations.html">vectorization</a></li>
<li><a class="reference external" href="https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/Broadcasting.html">broadcasting</a></li>
<li><a class="reference external" href="https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/BasicIndexing.html">basic and advanced indexing</a> (including all varieties of mixed indexing schemes) for both getting and setting items.</li>
<li>fully-fledged support for <a class="reference external" href="https://rockt.github.io/2018/04/30/einsum">einsum</a> (including broadcasting and traces,
which are not supported by PyTorch, TensorFlow, or HIPS-autograd)</li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">mygrad.Tensor</span></code> plays nicely with NumPy-arrays, which behave as constants when they are used in computational graphs:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">mg</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="n">y</span>  <span class="c1"># (2 ** 1, 2 ** 2, 2 ** 3)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">grad</span>
<span class="go">array([ 1.,  4., 12.])</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">mygrad.nn</span></code> supplies essential functions for machine learning, including:</p>
<ul class="simple">
<li>N-dimensional convolutions (with striding, dilation, and padding)</li>
<li>N-dimensional pooling</li>
<li>A <a class="reference external" href="https://en.wikipedia.org/wiki/Gated_recurrent_unit">gated recurrent unit</a> for sequence-learning (with input-level
dropout and variational hidden-hidden dropout)</li>
</ul>
<p>It leverages a nice <a class="reference external" href="https://github.com/rsokl/MyGrad/blob/a72ebc26acf5c254f59a562c8045698387763a41/mygrad/nnet/layers/utils.py#L6">sliding window
view</a>
function, which produces convolution-style windowed views of arrays/tensors without making copies of them, to
intuitively (and quite efficiently) perform the neural network-style convolutions and pooling.</p>
</div>
<div class="section" id="advanced-example">
<span id="advanced-example"></span><h2>Advanced Example<a class="headerlink" href="#advanced-example" title="Permalink to this headline">¶</a></h2>
<p>The following is an example of using <code class="docutils literal notranslate"><span class="pre">mygrad</span></code> to compute the <a class="reference external" href="https://en.wikipedia.org/wiki/Hinge_loss">hinge loss</a> of classification scores and to “backpropagate” through (compute the gradient of) this loss. This example demonstrates some of mygrad’s ability to perform backpropagation through broadcasted operations, basic indexing, advanced indexing, and in-place assignments.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mygrad</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">class_scores</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">10</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>         <span class="c1"># 100 samples, 10 possible classes for each</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">class_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>  <span class="c1"># correct label for each datum</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">class_labels</span> <span class="o">=</span> <span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">class_labels</span><span class="p">)),</span> <span class="n">class_labels</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">correct_class_scores</span> <span class="o">=</span> <span class="n">class_scores</span><span class="p">[</span><span class="n">class_labels</span><span class="p">]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">Lij</span> <span class="o">=</span> <span class="n">class_scores</span> <span class="o">-</span> <span class="n">correct_class_scores</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1.</span>  <span class="c1"># 100x10 margins</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Lij</span><span class="p">[</span><span class="n">Lij</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>      <span class="c1"># scores within the hinge incur no loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Lij</span><span class="p">[</span><span class="n">class_labels</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># the score corresponding to the correct label incurs no loss</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">Lij</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">class_scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># compute mean hinge loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>    <span class="c1"># compute gradient of loss w.r.t all dependent tensors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">class_scores</span><span class="o">.</span><span class="n">grad</span>  <span class="c1"># d(loss)/d(class_scores)</span>
<span class="go">array([[ 0.  ,  0.01,  0.  , -0.04,  0.  ,  0.  ,  0.01,  0.  ,  0.01, 0.01], ...])</span>
</pre></div>
</div>
</div>
<div class="section" id="computational-graph-visualization">
<span id="computational-graph-visualization"></span><h2>Computational Graph Visualization<a class="headerlink" href="#computational-graph-visualization" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">mygrad</span></code> uses <a class="reference external" href="http://www.graphviz.org">Graphviz</a> and a <a class="reference external" href="https://graphviz.readthedocs.io/en/stable/">Python interface for Graphviz</a> to render the computational graphs built using tensors. These graphs can be rendered in Jupyter notebooks, allowing for quick checks of graph structure, or can be saved to file for later reference.</p>
<p>The dependencies can be installed with:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>conda install graphviz
conda install python-graphviz
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Ryan Soklaski

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>